{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 3: Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming assignment, you will implement logistic regression to predict the sentiment of reviews that come from `imdb.com`, `amazon.com`, and `yelp.com`. Make sure the notebook is in the same folder that contains `full_set.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set consists of 3000 sentences, each labeled '1' (if it came from a positive review) or '0' (if it came from a negative review). To be consistent with our notation from lecture, we will change the negative review label to '-1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  -1 ;  So there is no way for me to plug it in here in the US unless I go by a converter.\n",
      "Label:  1 ;  Good case, Excellent value.\n",
      "Label:  1 ;  Great for the jawbone.\n",
      "Label:  -1 ;  Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!\n",
      "Label:  1 ;  The mic is great.\n"
     ]
    }
   ],
   "source": [
    "## Read in the data set.\n",
    "with open(\"full_set.txt\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "## Remove leading and trailing white space\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "## Separate the sentences from the labels\n",
    "sentences = [x.split(\"\\t\")[0] for x in content]\n",
    "labels = [x.split(\"\\t\")[1] for x in content]\n",
    "\n",
    "## Transform the labels from '0 v.s. 1' to '-1 v.s. 1'\n",
    "y = np.array(labels, dtype='int8')\n",
    "y = 2*y - 1\n",
    "\n",
    "for i in range(5):\n",
    "    print ('Label: ',y[i],'; ', sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text data\n",
    "\n",
    "To transform this prediction problem into one amenable to linear classification, we will first need to preprocess the text data. We will do four transformations:\n",
    "\n",
    "1. Remove punctuation and numbers.\n",
    "2. Transform all words to lower-case.\n",
    "3. Remove _stop words_.\n",
    "4. Convert the sentences into vectors, using a bag-of-words representation.\n",
    "\n",
    "We begin with first two steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## full_remove takes a string x and a list of characters removal_list \n",
    "## returns x with all the characters in removal_list replaced by ' '\n",
    "def full_remove(x, removal_list):\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x\n",
    "\n",
    "## Remove digits\n",
    "digits = [str(x) for x in range(10)]\n",
    "digit_less = [full_remove(x, digits) for x in sentences]\n",
    "\n",
    "## Remove punctuation\n",
    "punc_less = [full_remove(x, list(string.punctuation)) for x in digit_less]\n",
    "\n",
    "## Make everything lower-case\n",
    "sents_lower = [x.lower() for x in punc_less]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "\n",
    "Stop words are words that are filtered out because they are believed to contain no useful information for the task at hand. These usually include articles such as 'a' and 'the', pronouns such as 'i' and 'they', and prepositions such 'to' and 'from'. We have put together a very small list of stop words, but these are by no means comprehensive. Feel free to use something different; for instance, larger lists can easily be found on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define our stop words\n",
    "stop_set = set(['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from'])\n",
    "\n",
    "## Remove stop words\n",
    "sents_split = [x.split() for x in sents_lower]\n",
    "sents_processed = [\" \".join(list(filter(lambda a: a not in stop_set, x))) for x in sents_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the sentences look like so far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so there is no way for me plug in here in us unless go by converter',\n",
       " 'good case excellent value',\n",
       " 'great for jawbone',\n",
       " 'tied charger for conversations lasting more than minutes major problems',\n",
       " 'mic is great',\n",
       " 'have jiggle plug get line up right get decent volume',\n",
       " 'if you have several dozen or several hundred contacts then imagine fun sending each them one by one',\n",
       " 'if you are razr owner you must have this',\n",
       " 'needless say wasted my money',\n",
       " 'what waste money and time']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_processed[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "\n",
    "In order to use linear classifiers on our data set, we need to transform our textual data into numeric data. The classical way to do this is known as the _bag of words_ representation. In this representation, each word is thought of as corresponding to a number in `{1, 2, ..., d}` where `d` is the size of our vocabulary. And each sentence is represented as a d-dimensional vector $x$, where $x_i$ is the number of times that word $i$ occurs in the sentence.\n",
    "\n",
    "To do this transformation, we will make use of the `CountVectorizer` class in `scikit-learn` (Note that this is the only time you can call an external function from scikit-learn). We will cap the number of features at 4500, meaning a word will make it into our vocabulary only if it is one of the 4500 most common words in the corpus. This is often a useful step as it can weed out spelling mistakes and words which occur too infrequently to be useful.\n",
    "\n",
    "**Task P1:** Once you get the bag-of-words representation, append a '1' to the beginning of each vector to allow our linear classifier to learn a bias term. What is the size of the resulting data_mat matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original size:  (3000, 4500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## Transform to bag of words representation.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 4500)\n",
    "data_features = vectorizer.fit_transform(sents_processed)\n",
    "print ('The original size: ',data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated size:  (3000, 4501)\n"
     ]
    }
   ],
   "source": [
    "## STUDENT: YOUR CODE STARTS HERE\n",
    "# Task: Append '1' to the beginning of each vector.\n",
    "# Hint: You can use data_features.toarray() to transform data_features into a numpy array\n",
    "# The output should be a numpy array named data_mat\n",
    "\n",
    "extra_one = np.ones((3000,1), dtype=float)\n",
    "data_mat = np.concatenate((extra_one, data_features.toarray()), axis=1)\n",
    "\n",
    "## STUDENT: CODE ENDS\n",
    "print ('The updated size: ',data_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / test split\n",
    "\n",
    "Finally, we split the data into a training set of 2500 sentences and a test set of 500 sentences (of which 250 are positive and 250 negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:  (2500, 4501)\n",
      "test data:  (500, 4501)\n"
     ]
    }
   ],
   "source": [
    "## Split the data into testing and training sets\n",
    "np.random.seed(0)\n",
    "test_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_inds = list(set(range(len(labels))) - set(test_inds))\n",
    "\n",
    "train_data = data_mat[train_inds,]\n",
    "train_labels = y[train_inds]\n",
    "\n",
    "test_data = data_mat[test_inds,]\n",
    "test_labels = y[test_inds]\n",
    "\n",
    "print(\"train data: \", train_data.shape)\n",
    "print(\"test data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fitting a logistic regression model to the training data\n",
    "\n",
    "In this section, we will implement our own logistic regression solver using gradient descent. As we have seen in the class, to learn the parameters of logistic regression, we need to perform the following optimization:\n",
    "\n",
    "$$\n",
    "{\\bf \\tilde\\theta}_t =\n",
    "\\underset{{\\bf \\tilde\\theta}}{\\operatorname{argmin}} \\; L_\\mathcal{D}({\\bf \\tilde\\theta}) =\n",
    "\\underset{{\\bf \\tilde\\theta}}{\\operatorname{argmin}} \\;\\sum_{i=1}^{n} \\ln \\left( 1 + e^{y_i \\; {\\bf \\tilde\\theta}_t^T {\\tilde x}_i} \\right)\n",
    "$$\n",
    "where $y_i\\in\\{-1,+1\\}$ is the label, ${\\bf \\tilde\\theta}$ is the vector of coefficients:\n",
    "$$\n",
    "{\\bf \\tilde\\theta} = \\begin{bmatrix} \\theta_0 & \\theta_1 & ... & \\theta_d \\end{bmatrix}^T,\n",
    "$$\n",
    "and ${\\tilde x}$ is the \"augmented\" feature vector (of $d+1$ dimensions), where we stick a 1 in the front of the original features:\n",
    "$$\n",
    "{\\tilde x} = \\begin{bmatrix} 1 & x_1 & ... & x_d \\end{bmatrix}^T.\n",
    "$$\n",
    "\n",
    "\n",
    "There is no nice, closed-form solution like with [least-squares linear regression](http://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse) so we will use [gradient descent](http://en.wikipedia.org/wiki/Gradient_descent) instead. Specifically we will use batch gradient descent which calculates the gradient from all data points in the data set. Luckily, the loss function $L_\\mathcal{D}({\\bf \\tilde\\theta})$ we want to minimize is [convex](http://en.wikipedia.org/wiki/Convex_optimization) so there is only one minimum. Thus the minimum we arrive at is the global minimum.\n",
    "\n",
    "Gradient descent is a general method and requires twice differentiability for [smoothness](http://en.wikipedia.org/wiki/Smooth_function). It updates the parameters using a first-order approximation of the error surface.\n",
    "\n",
    "$$\n",
    "{\\bf \\tilde\\theta}_{t+1} = {\\bf \\tilde\\theta}_t + \\nabla L_\\mathcal{D}({\\bf \\tilde\\theta}_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task P2:** Derive the gradient of the loss $L_\\mathcal{D}({\\bf \\tilde\\theta})$ with respect to ${\\bf \\tilde\\theta}$, namely $\\nabla L_\\mathcal{D}({\\bf \\tilde\\theta}_t)$. The answer should depend on data points $(x_i,y_i)$ for $i=1,...,n$, and the model parameter ${\\bf \\tilde\\theta}$. Make sure you get the sign correct. Also implement the function `weight_derivative`. Print the output of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_derivative(weights, feature_matrix, labels):\n",
    "    # Input:\n",
    "    # weights: weight vector w, a numpy vector of dimension d\n",
    "    # feature_matrix: numpy array of size n by d, where n is the number of data points, and d is the feature dimension\n",
    "    # labels: true labels y, a numpy vector of dimension d, each with value -1 or +1\n",
    "    # Output:\n",
    "    # Derivative of the regression cost function with respect to the weight w, a numpy array of dimension d\n",
    "        \n",
    "    ## STUDENT: Start of code ###\n",
    "    derivative_sum = 0.\n",
    "    for i in range(len(labels)):\n",
    "        outside = feature_matrix[i] * labels[i]\n",
    "        inside = np.exp(-1 * labels[i] * np.dot(weights, feature_matrix[i]) )\n",
    "        derivative_sum += outside * (1 / ( 1 + inside))\n",
    "    \n",
    "        \n",
    "    return derivative_sum\n",
    "    # End of code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.23407200e+03  9.99999959e-01 -4.24835426e-18 -6.14417460e-06\n",
      "  1.99987630e+00  2.99985907e+00 -1.79862100e-02  3.49776173e+01\n",
      "  1.99996625e+00  1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# STUDENT: PRINT THE OUTPUT AND COPY IT TO THE SOLUTION FILE\n",
    "my_weights = np.ones(data_mat.shape[1]) # a weight of all 1s\n",
    "derivative = weight_derivative(my_weights,train_data,train_labels)\n",
    "\n",
    "print (derivative[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can just use the same gradient descent algorithm that we wrote in assignment 2 to solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(feature_matrix, labels, initial_weights, step_size, tolerance):\n",
    "    # Gradient descent algorithm for logistic regression problem    \n",
    "    \n",
    "    # Input:\n",
    "    # feature_matrix: numpy array of size n by d, where n is the number of data points, and d is the feature dimension\n",
    "    # labels: true labels y, a numpy vector of dimension n\n",
    "    # initial_weights: initial weight vector to start with, a numpy vector of dimension d\n",
    "    # step_size: step size of update\n",
    "    # tolerance: tolerace epsilon for stopping condition\n",
    "    # Output:\n",
    "    # Weights obtained after convergence\n",
    "\n",
    "    converged = False \n",
    "    weights = np.array(initial_weights) # current iterate\n",
    "    i = 0\n",
    "    while not converged:\n",
    "        # impelementation of what the gradient descent algorithm does in every iteration\n",
    "        # Refer back to the update rule listed above: update the weight\n",
    "        i += 1\n",
    "        derivative = weight_derivative(weights, feature_matrix, labels)\n",
    "        \n",
    "        weights -= (step_size * derivative)\n",
    "        \n",
    "        # Compute the gradient magnitude:\n",
    "        \n",
    "        gradient_magnitude = np.sqrt(np.sum(derivative**2))\n",
    "        \n",
    "        # Check the stopping condition to decide whether you want to stop the iterations\n",
    "        # print (\"grad mag :\", gradient_magnitude)\n",
    "        #print (\"tolerance:\", tolerance)\n",
    "        \n",
    "        if gradient_magnitude < tolerance:\n",
    "            converged = True\n",
    "        \n",
    "        print (\"Iteration: \",i,\"gradient_magnitude: \", gradient_magnitude) # for us to check about convergence\n",
    "        \n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task P3:** Specify the initial_weights, step_size, and tolerance for the function `gradient_descent`. Copy the outputs of the code to the solution file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1 gradient_magnitude:  209.2175900826697\n",
      "Iteration:  2 gradient_magnitude:  212.64421867541415\n",
      "Iteration:  3 gradient_magnitude:  584.1017850068945\n",
      "Iteration:  4 gradient_magnitude:  1256.785468117014\n",
      "Iteration:  5 gradient_magnitude:  1536.9148922237887\n",
      "Iteration:  6 gradient_magnitude:  1409.1836051491587\n",
      "Iteration:  7 gradient_magnitude:  1458.5670556063646\n",
      "Iteration:  8 gradient_magnitude:  1377.5557284445008\n",
      "Iteration:  9 gradient_magnitude:  1300.7306897048818\n",
      "Iteration:  10 gradient_magnitude:  1275.3901995059225\n",
      "Iteration:  11 gradient_magnitude:  1210.8689930956264\n",
      "Iteration:  12 gradient_magnitude:  1183.3122667955197\n",
      "Iteration:  13 gradient_magnitude:  1143.7827656559937\n",
      "Iteration:  14 gradient_magnitude:  1123.2213579344218\n",
      "Iteration:  15 gradient_magnitude:  1097.5395865103742\n",
      "Iteration:  16 gradient_magnitude:  1081.8619393447846\n",
      "Iteration:  17 gradient_magnitude:  1061.382551300502\n",
      "Iteration:  18 gradient_magnitude:  1047.2783473847555\n",
      "Iteration:  19 gradient_magnitude:  1028.33031960631\n",
      "Iteration:  20 gradient_magnitude:  1014.9038320157325\n",
      "Iteration:  21 gradient_magnitude:  996.8935564366525\n",
      "Iteration:  22 gradient_magnitude:  984.1076780231133\n",
      "Iteration:  23 gradient_magnitude:  967.0239088547238\n",
      "Iteration:  24 gradient_magnitude:  954.7326610353883\n",
      "Iteration:  25 gradient_magnitude:  938.4105340162379\n",
      "Iteration:  26 gradient_magnitude:  926.4127965970251\n",
      "Iteration:  27 gradient_magnitude:  910.6622678459509\n",
      "Iteration:  28 gradient_magnitude:  898.7848057155093\n",
      "Iteration:  29 gradient_magnitude:  883.460324055539\n",
      "Iteration:  30 gradient_magnitude:  871.5694147684499\n",
      "Iteration:  31 gradient_magnitude:  856.567704891846\n",
      "Iteration:  32 gradient_magnitude:  844.5740166434672\n",
      "Iteration:  33 gradient_magnitude:  829.8284196486136\n",
      "Iteration:  34 gradient_magnitude:  817.6958386292453\n",
      "Iteration:  35 gradient_magnitude:  803.1757952524371\n",
      "Iteration:  36 gradient_magnitude:  790.9223179043909\n",
      "Iteration:  37 gradient_magnitude:  776.630511513173\n",
      "Iteration:  38 gradient_magnitude:  764.3140257167513\n",
      "Iteration:  39 gradient_magnitude:  750.2757801180821\n",
      "Iteration:  40 gradient_magnitude:  737.9689018841347\n",
      "Iteration:  41 gradient_magnitude:  724.2151793922668\n",
      "Iteration:  42 gradient_magnitude:  711.9793910578471\n",
      "Iteration:  43 gradient_magnitude:  698.5296323784795\n",
      "Iteration:  44 gradient_magnitude:  686.3980477603345\n",
      "Iteration:  45 gradient_magnitude:  673.2493328386929\n",
      "Iteration:  46 gradient_magnitude:  661.2221773920269\n",
      "Iteration:  47 gradient_magnitude:  648.3476589986032\n",
      "Iteration:  48 gradient_magnitude:  636.39876527527\n",
      "Iteration:  49 gradient_magnitude:  623.7539787363194\n",
      "Iteration:  50 gradient_magnitude:  611.8430140899121\n",
      "Iteration:  51 gradient_magnitude:  599.3758986510311\n",
      "Iteration:  52 gradient_magnitude:  587.4606109236814\n",
      "Iteration:  53 gradient_magnitude:  575.1208277085286\n",
      "Iteration:  54 gradient_magnitude:  563.1656094899296\n",
      "Iteration:  55 gradient_magnitude:  550.9104947902235\n",
      "Iteration:  56 gradient_magnitude:  538.8903583561545\n",
      "Iteration:  57 gradient_magnitude:  526.6871727888521\n",
      "Iteration:  58 gradient_magnitude:  514.5882505547553\n",
      "Iteration:  59 gradient_magnitude:  502.4139463958696\n",
      "Iteration:  60 gradient_magnitude:  490.232290830791\n",
      "Iteration:  61 gradient_magnitude:  478.07237403375234\n",
      "Iteration:  62 gradient_magnitude:  465.81249656443856\n",
      "Iteration:  63 gradient_magnitude:  453.66010817673754\n",
      "Iteration:  64 gradient_magnitude:  441.33404720918537\n",
      "Iteration:  65 gradient_magnitude:  429.189750161746\n",
      "Iteration:  66 gradient_magnitude:  416.81690556319415\n",
      "Iteration:  67 gradient_magnitude:  404.6891726768411\n",
      "Iteration:  68 gradient_magnitude:  392.2967629528769\n",
      "Iteration:  69 gradient_magnitude:  380.20285727512345\n",
      "Iteration:  70 gradient_magnitude:  367.8266013077073\n",
      "Iteration:  71 gradient_magnitude:  355.7933744864988\n",
      "Iteration:  72 gradient_magnitude:  343.4778696356579\n",
      "Iteration:  73 gradient_magnitude:  331.5419771529183\n",
      "Iteration:  74 gradient_magnitude:  319.3402843144899\n",
      "Iteration:  75 gradient_magnitude:  307.547451069085\n",
      "Iteration:  76 gradient_magnitude:  295.5196197702691\n",
      "Iteration:  77 gradient_magnitude:  283.9228620287215\n",
      "Iteration:  78 gradient_magnitude:  272.1334663315304\n",
      "Iteration:  79 gradient_magnitude:  260.79049122631613\n",
      "Iteration:  80 gradient_magnitude:  249.30556587089515\n",
      "Iteration:  81 gradient_magnitude:  238.27580186415088\n",
      "Iteration:  82 gradient_magnitude:  227.15973799616583\n",
      "Iteration:  83 gradient_magnitude:  216.50151181862836\n",
      "Iteration:  84 gradient_magnitude:  205.81443658149323\n",
      "Iteration:  85 gradient_magnitude:  195.58269942865454\n",
      "Iteration:  86 gradient_magnitude:  185.37866330248985\n",
      "Iteration:  87 gradient_magnitude:  175.6234522413175\n",
      "Iteration:  88 gradient_magnitude:  165.949485027442\n",
      "Iteration:  89 gradient_magnitude:  156.71508388863742\n",
      "Iteration:  90 gradient_magnitude:  147.6109607604349\n",
      "Iteration:  91 gradient_magnitude:  138.93557487588703\n",
      "Iteration:  92 gradient_magnitude:  130.43402011517728\n",
      "Iteration:  93 gradient_magnitude:  122.34972918882\n",
      "Iteration:  94 gradient_magnitude:  114.47678260333875\n",
      "Iteration:  95 gradient_magnitude:  107.00956998066466\n",
      "Iteration:  96 gradient_magnitude:  99.7849149498983\n",
      "Iteration:  97 gradient_magnitude:  92.95465087529884\n",
      "Iteration:  98 gradient_magnitude:  86.39180487249278\n",
      "Iteration:  99 gradient_magnitude:  80.21214450534154\n",
      "Iteration:  100 gradient_magnitude:  74.3184991614723\n",
      "Iteration:  101 gradient_magnitude:  68.79670932423879\n",
      "Iteration:  102 gradient_magnitude:  63.57344575338471\n",
      "Iteration:  103 gradient_magnitude:  58.710172824901846\n",
      "Iteration:  104 gradient_magnitude:  54.15204784162417\n",
      "Iteration:  105 gradient_magnitude:  49.94096659989784\n",
      "Iteration:  106 gradient_magnitude:  46.03585808105781\n",
      "Iteration:  107 gradient_magnitude:  42.463000360195224\n",
      "Iteration:  108 gradient_magnitude:  39.190941857163686\n",
      "Iteration:  109 gradient_magnitude:  36.23335805792003\n",
      "Iteration:  110 gradient_magnitude:  33.5647082585785\n",
      "Iteration:  111 gradient_magnitude:  31.18816887291432\n",
      "Iteration:  112 gradient_magnitude:  29.080848312511723\n",
      "Iteration:  113 gradient_magnitude:  27.236912176292073\n",
      "Iteration:  114 gradient_magnitude:  25.63364343732374\n",
      "Iteration:  115 gradient_magnitude:  24.257755625097204\n",
      "Iteration:  116 gradient_magnitude:  23.08567152954206\n",
      "Iteration:  117 gradient_magnitude:  22.099042437343662\n",
      "Iteration:  118 gradient_magnitude:  21.274205496622386\n",
      "Iteration:  119 gradient_magnitude:  20.59102839853628\n",
      "Iteration:  120 gradient_magnitude:  20.027728082421643\n",
      "Iteration:  121 gradient_magnitude:  19.56561037098103\n",
      "Iteration:  122 gradient_magnitude:  19.186604935644556\n",
      "Iteration:  123 gradient_magnitude:  18.87551562261523\n",
      "Iteration:  124 gradient_magnitude:  18.61875917197336\n",
      "Iteration:  125 gradient_magnitude:  18.40522000962763\n",
      "Iteration:  126 gradient_magnitude:  18.22550672671518\n",
      "Iteration:  127 gradient_magnitude:  18.072132886222924\n",
      "Iteration:  128 gradient_magnitude:  17.939031838719774\n",
      "Iteration:  129 gradient_magnitude:  17.821478528163816\n",
      "Iteration:  130 gradient_magnitude:  17.715764573370677\n",
      "Iteration:  131 gradient_magnitude:  17.619060008863656\n",
      "Iteration:  132 gradient_magnitude:  17.529199605740764\n",
      "Iteration:  133 gradient_magnitude:  17.444560587446784\n",
      "Iteration:  134 gradient_magnitude:  17.363926788294005\n",
      "Iteration:  135 gradient_magnitude:  17.286400953639944\n",
      "Iteration:  136 gradient_magnitude:  17.211321593940664\n",
      "Iteration:  137 gradient_magnitude:  17.13820676372649\n",
      "Iteration:  138 gradient_magnitude:  17.06670508791991\n",
      "Iteration:  139 gradient_magnitude:  16.996562377607162\n",
      "Iteration:  140 gradient_magnitude:  16.927593858762798\n",
      "Iteration:  141 gradient_magnitude:  16.859665518711925\n",
      "Iteration:  142 gradient_magnitude:  16.792678929177484\n",
      "Iteration:  143 gradient_magnitude:  16.7265613666857\n",
      "Iteration:  144 gradient_magnitude:  16.661257804776092\n",
      "Iteration:  145 gradient_magnitude:  16.59672593834993\n",
      "Iteration:  146 gradient_magnitude:  16.53293209198693\n",
      "Iteration:  147 gradient_magnitude:  16.469848834954345\n",
      "Iteration:  148 gradient_magnitude:  16.40745294637519\n",
      "Iteration:  149 gradient_magnitude:  16.34572432735134\n",
      "Iteration:  150 gradient_magnitude:  16.284645008023983\n",
      "Iteration:  151 gradient_magnitude:  16.22419867511495\n",
      "Iteration:  152 gradient_magnitude:  16.16437019030453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  153 gradient_magnitude:  16.10514539367519\n",
      "Iteration:  154 gradient_magnitude:  16.046510866880503\n",
      "Iteration:  155 gradient_magnitude:  15.988453852754946\n",
      "Iteration:  156 gradient_magnitude:  15.930962133994601\n",
      "Iteration:  157 gradient_magnitude:  15.87402399820974\n",
      "Iteration:  158 gradient_magnitude:  15.817628171145916\n",
      "Iteration:  159 gradient_magnitude:  15.761763797976469\n",
      "Iteration:  160 gradient_magnitude:  15.706420402823937\n",
      "Iteration:  161 gradient_magnitude:  15.651587875259974\n",
      "Iteration:  162 gradient_magnitude:  15.59725644310102\n",
      "Iteration:  163 gradient_magnitude:  15.543416660465736\n",
      "Iteration:  164 gradient_magnitude:  15.4900593877519\n",
      "Iteration:  165 gradient_magnitude:  15.437175780320262\n",
      "Iteration:  166 gradient_magnitude:  15.384757272702528\n",
      "Iteration:  167 gradient_magnitude:  15.33279556783401\n",
      "Iteration:  168 gradient_magnitude:  15.281282623986028\n",
      "Iteration:  169 gradient_magnitude:  15.230210644651683\n",
      "Iteration:  170 gradient_magnitude:  15.179572067381049\n",
      "Iteration:  171 gradient_magnitude:  15.129359554404584\n",
      "Iteration:  172 gradient_magnitude:  15.079565982890676\n",
      "Iteration:  173 gradient_magnitude:  15.030184436335034\n",
      "Iteration:  174 gradient_magnitude:  14.9812081959398\n",
      "Iteration:  175 gradient_magnitude:  14.932630732752292\n",
      "Iteration:  176 gradient_magnitude:  14.884445699964864\n",
      "Iteration:  177 gradient_magnitude:  14.836646925759693\n",
      "Iteration:  178 gradient_magnitude:  14.789228406386957\n",
      "Iteration:  179 gradient_magnitude:  14.742184299660112\n",
      "Iteration:  180 gradient_magnitude:  14.695508918705679\n",
      "Iteration:  181 gradient_magnitude:  14.649196726050144\n",
      "Iteration:  182 gradient_magnitude:  14.603242327957602\n",
      "Iteration:  183 gradient_magnitude:  14.557640469051073\n",
      "Iteration:  184 gradient_magnitude:  14.512386027169724\n",
      "Iteration:  185 gradient_magnitude:  14.467474008471374\n",
      "Iteration:  186 gradient_magnitude:  14.422899542752038\n",
      "Iteration:  187 gradient_magnitude:  14.378657878981288\n",
      "Iteration:  188 gradient_magnitude:  14.334744381035113\n",
      "Iteration:  189 gradient_magnitude:  14.291154523620724\n",
      "Iteration:  190 gradient_magnitude:  14.247883888380223\n",
      "Iteration:  191 gradient_magnitude:  14.204928160166121\n",
      "Iteration:  192 gradient_magnitude:  14.162283123478563\n",
      "Iteration:  193 gradient_magnitude:  14.119944659057047\n",
      "Iteration:  194 gradient_magnitude:  14.077908740618357\n",
      "Iteration:  195 gradient_magnitude:  14.0361714317338\n",
      "Iteration:  196 gradient_magnitude:  13.994728882838668\n",
      "Iteration:  197 gradient_magnitude:  13.953577328367645\n",
      "Iteration:  198 gradient_magnitude:  13.9127130840099\n",
      "Iteration:  199 gradient_magnitude:  13.872132544078209\n",
      "Iteration:  200 gradient_magnitude:  13.831832178986582\n",
      "Iteration:  201 gradient_magnitude:  13.791808532831256\n",
      "Iteration:  202 gradient_magnitude:  13.752058221070152\n",
      "Iteration:  203 gradient_magnitude:  13.712577928296206\n",
      "Iteration:  204 gradient_magnitude:  13.67336440610013\n",
      "Iteration:  205 gradient_magnitude:  13.63441447101851\n",
      "Iteration:  206 gradient_magnitude:  13.595725002563249\n",
      "Iteration:  207 gradient_magnitude:  13.557292941328651\n",
      "Iteration:  208 gradient_magnitude:  13.519115287172589\n",
      "Iteration:  209 gradient_magnitude:  13.4811890974684\n",
      "Iteration:  210 gradient_magnitude:  13.443511485424283\n",
      "Iteration:  211 gradient_magnitude:  13.406079618467201\n",
      "Iteration:  212 gradient_magnitude:  13.36889071668839\n",
      "Iteration:  213 gradient_magnitude:  13.331942051347713\n",
      "Iteration:  214 gradient_magnitude:  13.295230943434289\n",
      "Iteration:  215 gradient_magnitude:  13.258754762280883\n",
      "Iteration:  216 gradient_magnitude:  13.222510924229741\n",
      "Iteration:  217 gradient_magnitude:  13.186496891347577\n",
      "Iteration:  218 gradient_magnitude:  13.150710170187615\n",
      "Iteration:  219 gradient_magnitude:  13.115148310596648\n",
      "Iteration:  220 gradient_magnitude:  13.079808904565159\n",
      "Iteration:  221 gradient_magnitude:  13.044689585118686\n",
      "Iteration:  222 gradient_magnitude:  13.009788025248662\n",
      "Iteration:  223 gradient_magnitude:  12.975101936881057\n",
      "Iteration:  224 gradient_magnitude:  12.940629069881224\n",
      "Iteration:  225 gradient_magnitude:  12.90636721109345\n",
      "Iteration:  226 gradient_magnitude:  12.872314183413737\n",
      "Iteration:  227 gradient_magnitude:  12.83846784489444\n",
      "Iteration:  228 gradient_magnitude:  12.804826087879452\n",
      "Iteration:  229 gradient_magnitude:  12.771386838168674\n",
      "Iteration:  230 gradient_magnitude:  12.738148054210564\n",
      "Iteration:  231 gradient_magnitude:  12.705107726321613\n",
      "Iteration:  232 gradient_magnitude:  12.672263875931698\n",
      "Iteration:  233 gradient_magnitude:  12.6396145548542\n",
      "Iteration:  234 gradient_magnitude:  12.60715784457995\n",
      "Iteration:  235 gradient_magnitude:  12.574891855594025\n",
      "Iteration:  236 gradient_magnitude:  12.542814726714477\n",
      "Iteration:  237 gradient_magnitude:  12.510924624452157\n",
      "Iteration:  238 gradient_magnitude:  12.479219742390756\n",
      "Iteration:  239 gradient_magnitude:  12.447698300586337\n",
      "Iteration:  240 gradient_magnitude:  12.41635854498553\n",
      "Iteration:  241 gradient_magnitude:  12.385198746861706\n",
      "Iteration:  242 gradient_magnitude:  12.354217202268433\n",
      "Iteration:  243 gradient_magnitude:  12.323412231509545\n",
      "Iteration:  244 gradient_magnitude:  12.292782178625183\n",
      "Iteration:  245 gradient_magnitude:  12.262325410893217\n",
      "Iteration:  246 gradient_magnitude:  12.23204031834546\n",
      "Iteration:  247 gradient_magnitude:  12.201925313298126\n",
      "Iteration:  248 gradient_magnitude:  12.17197882989598\n",
      "Iteration:  249 gradient_magnitude:  12.142199323669699\n",
      "Iteration:  250 gradient_magnitude:  12.112585271105942\n",
      "Iteration:  251 gradient_magnitude:  12.083135169229662\n",
      "Iteration:  252 gradient_magnitude:  12.053847535198212\n",
      "Iteration:  253 gradient_magnitude:  12.024720905906841\n",
      "Iteration:  254 gradient_magnitude:  11.995753837605122\n",
      "Iteration:  255 gradient_magnitude:  11.966944905523967\n",
      "Iteration:  256 gradient_magnitude:  11.938292703512834\n",
      "Iteration:  257 gradient_magnitude:  11.909795843686748\n",
      "Iteration:  258 gradient_magnitude:  11.881452956082828\n",
      "Iteration:  259 gradient_magnitude:  11.853262688325957\n",
      "Iteration:  260 gradient_magnitude:  11.82522370530328\n",
      "Iteration:  261 gradient_magnitude:  11.79733468884725\n",
      "Iteration:  262 gradient_magnitude:  11.769594337426893\n",
      "Iteration:  263 gradient_magnitude:  11.74200136584705\n",
      "Iteration:  264 gradient_magnitude:  11.714554504955279\n",
      "Iteration:  265 gradient_magnitude:  11.687252501356223\n",
      "Iteration:  266 gradient_magnitude:  11.660094117133122\n",
      "Iteration:  267 gradient_magnitude:  11.633078129576296\n",
      "Iteration:  268 gradient_magnitude:  11.60620333091832\n",
      "Iteration:  269 gradient_magnitude:  11.579468528075711\n",
      "Iteration:  270 gradient_magnitude:  11.55287254239689\n",
      "Iteration:  271 gradient_magnitude:  11.526414209416217\n",
      "Iteration:  272 gradient_magnitude:  11.500092378613926\n",
      "Iteration:  273 gradient_magnitude:  11.473905913181746\n",
      "Iteration:  274 gradient_magnitude:  11.447853689794043\n",
      "Iteration:  275 gradient_magnitude:  11.421934598384304\n",
      "Iteration:  276 gradient_magnitude:  11.39614754192681\n",
      "Iteration:  277 gradient_magnitude:  11.370491436223311\n",
      "Iteration:  278 gradient_magnitude:  11.344965209694568\n",
      "Iteration:  279 gradient_magnitude:  11.319567803176618\n",
      "Iteration:  280 gradient_magnitude:  11.294298169721607\n",
      "Iteration:  281 gradient_magnitude:  11.269155274403055\n",
      "Iteration:  282 gradient_magnitude:  11.244138094125423\n",
      "Iteration:  283 gradient_magnitude:  11.219245617437867\n",
      "Iteration:  284 gradient_magnitude:  11.19447684435202\n",
      "Iteration:  285 gradient_magnitude:  11.169830786163732\n",
      "Iteration:  286 gradient_magnitude:  11.145306465278626\n",
      "Iteration:  287 gradient_magnitude:  11.120902915041349\n",
      "Iteration:  288 gradient_magnitude:  11.096619179568437\n",
      "Iteration:  289 gradient_magnitude:  11.0724543135847\n",
      "Iteration:  290 gradient_magnitude:  11.048407382262969\n",
      "Iteration:  291 gradient_magnitude:  11.024477461067203\n",
      "Iteration:  292 gradient_magnitude:  11.000663635598764\n",
      "Iteration:  293 gradient_magnitude:  10.976965001445864\n",
      "Iteration:  294 gradient_magnitude:  10.953380664036015\n",
      "Iteration:  295 gradient_magnitude:  10.929909738491489\n",
      "Iteration:  296 gradient_magnitude:  10.906551349487607\n",
      "Iteration:  297 gradient_magnitude:  10.88330463111387\n",
      "Iteration:  298 gradient_magnitude:  10.86016872673781\n",
      "Iteration:  299 gradient_magnitude:  10.837142788871489\n",
      "Iteration:  300 gradient_magnitude:  10.814225979040614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  301 gradient_magnitude:  10.79141746765614\n",
      "Iteration:  302 gradient_magnitude:  10.768716433888367\n",
      "Iteration:  303 gradient_magnitude:  10.746122065543398\n",
      "Iteration:  304 gradient_magnitude:  10.723633558941959\n",
      "Iteration:  305 gradient_magnitude:  10.701250118800488\n",
      "Iteration:  306 gradient_magnitude:  10.678970958114428\n",
      "Iteration:  307 gradient_magnitude:  10.656795298043704\n",
      "Iteration:  308 gradient_magnitude:  10.63472236780031\n",
      "Iteration:  309 gradient_magnitude:  10.612751404537931\n",
      "Iteration:  310 gradient_magnitude:  10.59088165324362\n",
      "Iteration:  311 gradient_magnitude:  10.569112366631389\n",
      "Iteration:  312 gradient_magnitude:  10.547442805037745\n",
      "Iteration:  313 gradient_magnitude:  10.525872236319092\n",
      "Iteration:  314 gradient_magnitude:  10.504399935750945\n",
      "Iteration:  315 gradient_magnitude:  10.483025185928943\n",
      "Iteration:  316 gradient_magnitude:  10.461747276671595\n",
      "Iteration:  317 gradient_magnitude:  10.440565504924727\n",
      "Iteration:  318 gradient_magnitude:  10.419479174667593\n",
      "Iteration:  319 gradient_magnitude:  10.398487596820623\n",
      "Iteration:  320 gradient_magnitude:  10.37759008915473\n",
      "Iteration:  321 gradient_magnitude:  10.3567859762022\n",
      "Iteration:  322 gradient_magnitude:  10.336074589169092\n",
      "Iteration:  323 gradient_magnitude:  10.315455265849101\n",
      "Iteration:  324 gradient_magnitude:  10.294927350538904\n",
      "Iteration:  325 gradient_magnitude:  10.274490193954913\n",
      "Iteration:  326 gradient_magnitude:  10.254143153151404\n",
      "Iteration:  327 gradient_magnitude:  10.233885591440037\n",
      "Iteration:  328 gradient_magnitude:  10.21371687831067\n",
      "Iteration:  329 gradient_magnitude:  10.193636389353502\n",
      "Iteration:  330 gradient_magnitude:  10.17364350618248\n",
      "Iteration:  331 gradient_magnitude:  10.15373761635995\n",
      "Iteration:  332 gradient_magnitude:  10.133918113322544\n",
      "Iteration:  333 gradient_magnitude:  10.114184396308245\n",
      "Iteration:  334 gradient_magnitude:  10.094535870284636\n",
      "Iteration:  335 gradient_magnitude:  10.074971945878291\n",
      "Iteration:  336 gradient_magnitude:  10.055492039305296\n",
      "Iteration:  337 gradient_magnitude:  10.036095572302862\n",
      "Iteration:  338 gradient_magnitude:  10.016781972062025\n",
      "Iteration:  339 gradient_magnitude:  9.997550671161404\n",
      "Iteration:  340 gradient_magnitude:  9.97840110750198\n",
      "Iteration:  341 gradient_magnitude:  9.95933272424292\n",
      "Iteration:  342 gradient_magnitude:  9.940344969738366\n",
      "Iteration:  343 gradient_magnitude:  9.921437297475215\n",
      "Iteration:  344 gradient_magnitude:  9.902609166011866\n",
      "Iteration:  345 gradient_magnitude:  9.883860038917874\n",
      "Iteration:  346 gradient_magnitude:  9.865189384714565\n",
      "Iteration:  347 gradient_magnitude:  9.846596676816517\n",
      "Iteration:  348 gradient_magnitude:  9.82808139347395\n",
      "Iteration:  349 gradient_magnitude:  9.809643017715976\n",
      "Iteration:  350 gradient_magnitude:  9.791281037294695\n",
      "Iteration:  351 gradient_magnitude:  9.772994944630144\n",
      "Iteration:  352 gradient_magnitude:  9.754784236756041\n",
      "Iteration:  353 gradient_magnitude:  9.736648415266366\n",
      "Iteration:  354 gradient_magnitude:  9.718586986262697\n",
      "Iteration:  355 gradient_magnitude:  9.700599460302342\n",
      "Iteration:  356 gradient_magnitude:  9.682685352347237\n",
      "Iteration:  357 gradient_magnitude:  9.664844181713573\n",
      "Iteration:  358 gradient_magnitude:  9.647075472022166\n",
      "Iteration:  359 gradient_magnitude:  9.629378751149547\n",
      "Iteration:  360 gradient_magnitude:  9.611753551179753\n",
      "Iteration:  361 gradient_magnitude:  9.594199408356827\n",
      "Iteration:  362 gradient_magnitude:  9.576715863037975\n",
      "Iteration:  363 gradient_magnitude:  9.55930245964742\n",
      "Iteration:  364 gradient_magnitude:  9.54195874663088\n",
      "Iteration:  365 gradient_magnitude:  9.52468427641074\n",
      "Iteration:  366 gradient_magnitude:  9.507478605341793\n",
      "Iteration:  367 gradient_magnitude:  9.490341293667676\n",
      "Iteration:  368 gradient_magnitude:  9.473271905477851\n",
      "Iteration:  369 gradient_magnitude:  9.456270008665243\n",
      "Iteration:  370 gradient_magnitude:  9.439335174884425\n",
      "Iteration:  371 gradient_magnitude:  9.422466979510414\n",
      "Iteration:  372 gradient_magnitude:  9.405665001598024\n",
      "Iteration:  373 gradient_magnitude:  9.388928823841775\n",
      "Iteration:  374 gradient_magnitude:  9.372258032536367\n",
      "Iteration:  375 gradient_magnitude:  9.355652217537683\n",
      "Iteration:  376 gradient_magnitude:  9.339110972224331\n",
      "Iteration:  377 gradient_magnitude:  9.322633893459706\n",
      "Iteration:  378 gradient_magnitude:  9.306220581554566\n",
      "Iteration:  379 gradient_magnitude:  9.289870640230115\n",
      "Iteration:  380 gradient_magnitude:  9.273583676581586\n",
      "Iteration:  381 gradient_magnitude:  9.257359301042301\n",
      "Iteration:  382 gradient_magnitude:  9.24119712734823\n",
      "Iteration:  383 gradient_magnitude:  9.225096772502999\n",
      "Iteration:  384 gradient_magnitude:  9.209057856743387\n",
      "Iteration:  385 gradient_magnitude:  9.19308000350526\n",
      "Iteration:  386 gradient_magnitude:  9.177162839389965\n",
      "Iteration:  387 gradient_magnitude:  9.161305994131162\n",
      "Iteration:  388 gradient_magnitude:  9.14550910056209\n",
      "Iteration:  389 gradient_magnitude:  9.129771794583265\n",
      "Iteration:  390 gradient_magnitude:  9.114093715130588\n",
      "Iteration:  391 gradient_magnitude:  9.098474504143876\n",
      "Iteration:  392 gradient_magnitude:  9.082913806535803\n",
      "Iteration:  393 gradient_magnitude:  9.067411270161225\n",
      "Iteration:  394 gradient_magnitude:  9.051966545786911\n",
      "Iteration:  395 gradient_magnitude:  9.036579287061661\n",
      "Iteration:  396 gradient_magnitude:  9.021249150486804\n",
      "Iteration:  397 gradient_magnitude:  9.005975795387064\n",
      "Iteration:  398 gradient_magnitude:  8.990758883881803\n",
      "Iteration:  399 gradient_magnitude:  8.975598080856622\n",
      "Iteration:  400 gradient_magnitude:  8.960493053935329\n",
      "Iteration:  401 gradient_magnitude:  8.945443473452235\n",
      "Iteration:  402 gradient_magnitude:  8.930449012424834\n",
      "Iteration:  403 gradient_magnitude:  8.915509346526783\n",
      "Iteration:  404 gradient_magnitude:  8.90062415406124\n",
      "Iteration:  405 gradient_magnitude:  8.885793115934531\n",
      "Iteration:  406 gradient_magnitude:  8.871015915630137\n",
      "Iteration:  407 gradient_magnitude:  8.856292239182997\n",
      "Iteration:  408 gradient_magnitude:  8.841621775154136\n",
      "Iteration:  409 gradient_magnitude:  8.827004214605594\n",
      "Iteration:  410 gradient_magnitude:  8.812439251075663\n",
      "Iteration:  411 gradient_magnitude:  8.797926580554423\n",
      "Iteration:  412 gradient_magnitude:  8.783465901459575\n",
      "Iteration:  413 gradient_magnitude:  8.769056914612571\n",
      "Iteration:  414 gradient_magnitude:  8.754699323215013\n",
      "Iteration:  415 gradient_magnitude:  8.740392832825352\n",
      "Iteration:  416 gradient_magnitude:  8.726137151335863\n",
      "Iteration:  417 gradient_magnitude:  8.71193198894988\n",
      "Iteration:  418 gradient_magnitude:  8.697777058159316\n",
      "Iteration:  419 gradient_magnitude:  8.68367207372244\n",
      "Iteration:  420 gradient_magnitude:  8.669616752641918\n",
      "Iteration:  421 gradient_magnitude:  8.655610814143113\n",
      "Iteration:  422 gradient_magnitude:  8.641653979652634\n",
      "Iteration:  423 gradient_magnitude:  8.627745972777145\n",
      "Iteration:  424 gradient_magnitude:  8.613886519282403\n",
      "Iteration:  425 gradient_magnitude:  8.600075347072556\n",
      "Iteration:  426 gradient_magnitude:  8.586312186169673\n",
      "Iteration:  427 gradient_magnitude:  8.572596768693508\n",
      "Iteration:  428 gradient_magnitude:  8.558928828841497\n",
      "Iteration:  429 gradient_magnitude:  8.54530810286898\n",
      "Iteration:  430 gradient_magnitude:  8.531734329069666\n",
      "Iteration:  431 gradient_magnitude:  8.518207247756289\n",
      "Iteration:  432 gradient_magnitude:  8.504726601241517\n",
      "Iteration:  433 gradient_magnitude:  8.49129213381904\n",
      "Iteration:  434 gradient_magnitude:  8.477903591744912\n",
      "Iteration:  435 gradient_magnitude:  8.46456072321907\n",
      "Iteration:  436 gradient_magnitude:  8.451263278367062\n",
      "Iteration:  437 gradient_magnitude:  8.438011009222008\n",
      "Iteration:  438 gradient_magnitude:  8.42480366970672\n",
      "Iteration:  439 gradient_magnitude:  8.411641015616052\n",
      "Iteration:  440 gradient_magnitude:  8.398522804599434\n",
      "Iteration:  441 gradient_magnitude:  8.385448796143592\n",
      "Iteration:  442 gradient_magnitude:  8.372418751555461\n",
      "Iteration:  443 gradient_magnitude:  8.3594324339453\n",
      "Iteration:  444 gradient_magnitude:  8.346489608209964\n",
      "Iteration:  445 gradient_magnitude:  8.333590041016386\n",
      "Iteration:  446 gradient_magnitude:  8.32073350078521\n",
      "Iteration:  447 gradient_magnitude:  8.307919757674632\n",
      "Iteration:  448 gradient_magnitude:  8.295148583564384\n",
      "Iteration:  449 gradient_magnitude:  8.282419752039914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  450 gradient_magnitude:  8.269733038376726\n",
      "Iteration:  451 gradient_magnitude:  8.257088219524885\n",
      "Iteration:  452 gradient_magnitude:  8.244485074093692\n",
      "Iteration:  453 gradient_magnitude:  8.23192338233652\n",
      "Iteration:  454 gradient_magnitude:  8.21940292613581\n",
      "Iteration:  455 gradient_magnitude:  8.206923488988233\n",
      "Iteration:  456 gradient_magnitude:  8.19448485598999\n",
      "Iteration:  457 gradient_magnitude:  8.18208681382229\n",
      "Iteration:  458 gradient_magnitude:  8.169729150736963\n",
      "Iteration:  459 gradient_magnitude:  8.15741165654223\n",
      "Iteration:  460 gradient_magnitude:  8.145134122588626\n",
      "Iteration:  461 gradient_magnitude:  8.132896341755051\n",
      "Iteration:  462 gradient_magnitude:  8.120698108434997\n",
      "Iteration:  463 gradient_magnitude:  8.108539218522887\n",
      "Iteration:  464 gradient_magnitude:  8.096419469400573\n",
      "Iteration:  465 gradient_magnitude:  8.084338659923965\n",
      "Iteration:  466 gradient_magnitude:  8.0722965904098\n",
      "Iteration:  467 gradient_magnitude:  8.060293062622554\n",
      "Iteration:  468 gradient_magnitude:  8.048327879761468\n",
      "Iteration:  469 gradient_magnitude:  8.036400846447732\n",
      "Iteration:  470 gradient_magnitude:  8.024511768711777\n",
      "Iteration:  471 gradient_magnitude:  8.012660453980716\n",
      "Iteration:  472 gradient_magnitude:  8.000846711065886\n",
      "Iteration:  473 gradient_magnitude:  7.989070350150549\n",
      "Iteration:  474 gradient_magnitude:  7.977331182777691\n",
      "Iteration:  475 gradient_magnitude:  7.965629021837951\n",
      "Iteration:  476 gradient_magnitude:  7.953963681557677\n",
      "Iteration:  477 gradient_magnitude:  7.942334977487101\n",
      "Iteration:  478 gradient_magnitude:  7.9307427264886226\n",
      "Iteration:  479 gradient_magnitude:  7.9191867467252175\n",
      "Iteration:  480 gradient_magnitude:  7.907666857648964\n",
      "Iteration:  481 gradient_magnitude:  7.896182879989675\n",
      "Iteration:  482 gradient_magnitude:  7.884734635743653\n",
      "Iteration:  483 gradient_magnitude:  7.8733219481625465\n",
      "Iteration:  484 gradient_magnitude:  7.861944641742322\n",
      "Iteration:  485 gradient_magnitude:  7.850602542212346\n",
      "Iteration:  486 gradient_magnitude:  7.839295476524576\n",
      "Iteration:  487 gradient_magnitude:  7.828023272842843\n",
      "Iteration:  488 gradient_magnitude:  7.816785760532266\n",
      "Iteration:  489 gradient_magnitude:  7.805582770148743\n",
      "Iteration:  490 gradient_magnitude:  7.794414133428565\n",
      "Iteration:  491 gradient_magnitude:  7.78327968327812\n",
      "Iteration:  492 gradient_magnitude:  7.772179253763701\n",
      "Iteration:  493 gradient_magnitude:  7.761112680101418\n",
      "Iteration:  494 gradient_magnitude:  7.750079798647193\n",
      "Iteration:  495 gradient_magnitude:  7.739080446886877\n",
      "Iteration:  496 gradient_magnitude:  7.72811446342644\n",
      "Iteration:  497 gradient_magnitude:  7.717181687982266\n",
      "Iteration:  498 gradient_magnitude:  7.706281961371539\n",
      "Iteration:  499 gradient_magnitude:  7.695415125502732\n",
      "Iteration:  500 gradient_magnitude:  7.684581023366167\n",
      "Iteration:  501 gradient_magnitude:  7.6737794990246915\n",
      "Iteration:  502 gradient_magnitude:  7.663010397604421\n",
      "Iteration:  503 gradient_magnitude:  7.652273565285586\n",
      "Iteration:  504 gradient_magnitude:  7.641568849293466\n",
      "Iteration:  505 gradient_magnitude:  7.630896097889398\n",
      "Iteration:  506 gradient_magnitude:  7.620255160361888\n",
      "Iteration:  507 gradient_magnitude:  7.609645887017794\n",
      "Iteration:  508 gradient_magnitude:  7.599068129173603\n",
      "Iteration:  509 gradient_magnitude:  7.588521739146786\n",
      "Iteration:  510 gradient_magnitude:  7.578006570247235\n",
      "Iteration:  511 gradient_magnitude:  7.567522476768785\n",
      "Iteration:  512 gradient_magnitude:  7.557069313980816\n",
      "Iteration:  513 gradient_magnitude:  7.546646938119935\n",
      "Iteration:  514 gradient_magnitude:  7.536255206381732\n",
      "Iteration:  515 gradient_magnitude:  7.525893976912624\n",
      "Iteration:  516 gradient_magnitude:  7.5155631088017705\n",
      "Iteration:  517 gradient_magnitude:  7.505262462073063\n",
      "Iteration:  518 gradient_magnitude:  7.494991897677196\n",
      "Iteration:  519 gradient_magnitude:  7.484751277483817\n",
      "Iteration:  520 gradient_magnitude:  7.474540464273733\n",
      "Iteration:  521 gradient_magnitude:  7.464359321731215\n",
      "Iteration:  522 gradient_magnitude:  7.454207714436354\n",
      "Iteration:  523 gradient_magnitude:  7.444085507857505\n",
      "Iteration:  524 gradient_magnitude:  7.433992568343792\n",
      "Iteration:  525 gradient_magnitude:  7.423928763117691\n",
      "Iteration:  526 gradient_magnitude:  7.413893960267676\n",
      "Iteration:  527 gradient_magnitude:  7.4038880287409405\n",
      "Iteration:  528 gradient_magnitude:  7.393910838336184\n",
      "Iteration:  529 gradient_magnitude:  7.383962259696468\n",
      "Iteration:  530 gradient_magnitude:  7.374042164302144\n",
      "Iteration:  531 gradient_magnitude:  7.3641504244638325\n",
      "Iteration:  532 gradient_magnitude:  7.3542869133154865\n",
      "Iteration:  533 gradient_magnitude:  7.344451504807513\n",
      "Iteration:  534 gradient_magnitude:  7.334644073699957\n",
      "Iteration:  535 gradient_magnitude:  7.324864495555749\n",
      "Iteration:  536 gradient_magnitude:  7.315112646734019\n",
      "Iteration:  537 gradient_magnitude:  7.3053884043834785\n",
      "Iteration:  538 gradient_magnitude:  7.295691646435851\n",
      "Iteration:  539 gradient_magnitude:  7.286022251599377\n",
      "Iteration:  540 gradient_magnitude:  7.276380099352374\n",
      "Iteration:  541 gradient_magnitude:  7.266765069936857\n",
      "Iteration:  542 gradient_magnitude:  7.257177044352218\n",
      "Iteration:  543 gradient_magnitude:  7.247615904348975\n",
      "Iteration:  544 gradient_magnitude:  7.238081532422558\n",
      "Iteration:  545 gradient_magnitude:  7.228573811807179\n",
      "Iteration:  546 gradient_magnitude:  7.219092626469741\n",
      "Iteration:  547 gradient_magnitude:  7.209637861103809\n",
      "Iteration:  548 gradient_magnitude:  7.200209401123645\n",
      "Iteration:  549 gradient_magnitude:  7.190807132658282\n",
      "Iteration:  550 gradient_magnitude:  7.181430942545677\n",
      "Iteration:  551 gradient_magnitude:  7.172080718326892\n",
      "Iteration:  552 gradient_magnitude:  7.162756348240359\n",
      "Iteration:  553 gradient_magnitude:  7.1534577212161645\n",
      "Iteration:  554 gradient_magnitude:  7.144184726870425\n",
      "Iteration:  555 gradient_magnitude:  7.134937255499679\n",
      "Iteration:  556 gradient_magnitude:  7.125715198075363\n",
      "Iteration:  557 gradient_magnitude:  7.116518446238315\n",
      "Iteration:  558 gradient_magnitude:  7.107346892293343\n",
      "Iteration:  559 gradient_magnitude:  7.098200429203841\n",
      "Iteration:  560 gradient_magnitude:  7.089078950586453\n",
      "Iteration:  561 gradient_magnitude:  7.079982350705791\n",
      "Iteration:  562 gradient_magnitude:  7.070910524469198\n",
      "Iteration:  563 gradient_magnitude:  7.061863367421565\n",
      "Iteration:  564 gradient_magnitude:  7.052840775740188\n",
      "Iteration:  565 gradient_magnitude:  7.043842646229688\n",
      "Iteration:  566 gradient_magnitude:  7.034868876316957\n",
      "Iteration:  567 gradient_magnitude:  7.025919364046174\n",
      "Iteration:  568 gradient_magnitude:  7.016994008073853\n",
      "Iteration:  569 gradient_magnitude:  7.0080927076639385\n",
      "Iteration:  570 gradient_magnitude:  6.9992153626829525\n",
      "Iteration:  571 gradient_magnitude:  6.990361873595185\n",
      "Iteration:  572 gradient_magnitude:  6.981532141457922\n",
      "Iteration:  573 gradient_magnitude:  6.972726067916732\n",
      "Iteration:  574 gradient_magnitude:  6.963943555200784\n",
      "Iteration:  575 gradient_magnitude:  6.955184506118213\n",
      "Iteration:  576 gradient_magnitude:  6.946448824051534\n",
      "Iteration:  577 gradient_magnitude:  6.93773641295309\n",
      "Iteration:  578 gradient_magnitude:  6.929047177340547\n",
      "Iteration:  579 gradient_magnitude:  6.9203810222924345\n",
      "Iteration:  580 gradient_magnitude:  6.911737853443723\n",
      "Iteration:  581 gradient_magnitude:  6.903117576981437\n",
      "Iteration:  582 gradient_magnitude:  6.8945200996403235\n",
      "Iteration:  583 gradient_magnitude:  6.885945328698551\n",
      "Iteration:  584 gradient_magnitude:  6.877393171973444\n",
      "Iteration:  585 gradient_magnitude:  6.868863537817273\n",
      "Iteration:  586 gradient_magnitude:  6.860356335113067\n",
      "Iteration:  587 gradient_magnitude:  6.851871473270477\n",
      "Iteration:  588 gradient_magnitude:  6.843408862221667\n",
      "Iteration:  589 gradient_magnitude:  6.834968412417255\n",
      "Iteration:  590 gradient_magnitude:  6.826550034822285\n",
      "Iteration:  591 gradient_magnitude:  6.818153640912231\n",
      "Iteration:  592 gradient_magnitude:  6.809779142669058\n",
      "Iteration:  593 gradient_magnitude:  6.80142645257729\n",
      "Iteration:  594 gradient_magnitude:  6.793095483620143\n",
      "Iteration:  595 gradient_magnitude:  6.784786149275674\n",
      "Iteration:  596 gradient_magnitude:  6.776498363512979\n",
      "Iteration:  597 gradient_magnitude:  6.768232040788414\n",
      "Iteration:  598 gradient_magnitude:  6.759987096041864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  599 gradient_magnitude:  6.751763444693035\n",
      "Iteration:  600 gradient_magnitude:  6.743561002637785\n",
      "Iteration:  601 gradient_magnitude:  6.735379686244495\n",
      "Iteration:  602 gradient_magnitude:  6.727219412350463\n",
      "Iteration:  603 gradient_magnitude:  6.719080098258336\n",
      "Iteration:  604 gradient_magnitude:  6.710961661732576\n",
      "Iteration:  605 gradient_magnitude:  6.7028640209959605\n",
      "Iteration:  606 gradient_magnitude:  6.694787094726107\n",
      "Iteration:  607 gradient_magnitude:  6.686730802052041\n",
      "Iteration:  608 gradient_magnitude:  6.678695062550784\n",
      "Iteration:  609 gradient_magnitude:  6.670679796243984\n",
      "Iteration:  610 gradient_magnitude:  6.662684923594569\n",
      "Iteration:  611 gradient_magnitude:  6.654710365503435\n",
      "Iteration:  612 gradient_magnitude:  6.64675604330616\n",
      "Iteration:  613 gradient_magnitude:  6.638821878769757\n",
      "Iteration:  614 gradient_magnitude:  6.630907794089449\n",
      "Iteration:  615 gradient_magnitude:  6.6230137118854735\n",
      "Iteration:  616 gradient_magnitude:  6.615139555199928\n",
      "Iteration:  617 gradient_magnitude:  6.607285247493622\n",
      "Iteration:  618 gradient_magnitude:  6.599450712642984\n",
      "Iteration:  619 gradient_magnitude:  6.591635874936973\n",
      "Iteration:  620 gradient_magnitude:  6.583840659074038\n",
      "Iteration:  621 gradient_magnitude:  6.576064990159096\n",
      "Iteration:  622 gradient_magnitude:  6.5683087937005284\n",
      "Iteration:  623 gradient_magnitude:  6.560571995607229\n",
      "Iteration:  624 gradient_magnitude:  6.552854522185652\n",
      "Iteration:  625 gradient_magnitude:  6.545156300136908\n",
      "Iteration:  626 gradient_magnitude:  6.537477256553872\n",
      "Iteration:  627 gradient_magnitude:  6.5298173189183295\n",
      "Iteration:  628 gradient_magnitude:  6.522176415098139\n",
      "Iteration:  629 gradient_magnitude:  6.51455447334442\n",
      "Iteration:  630 gradient_magnitude:  6.506951422288781\n",
      "Iteration:  631 gradient_magnitude:  6.499367190940554\n",
      "Iteration:  632 gradient_magnitude:  6.491801708684062\n",
      "Iteration:  633 gradient_magnitude:  6.484254905275917\n",
      "Iteration:  634 gradient_magnitude:  6.476726710842333\n",
      "Iteration:  635 gradient_magnitude:  6.469217055876466\n",
      "Iteration:  636 gradient_magnitude:  6.461725871235782\n",
      "Iteration:  637 gradient_magnitude:  6.454253088139445\n",
      "Iteration:  638 gradient_magnitude:  6.4467986381657285\n",
      "Iteration:  639 gradient_magnitude:  6.439362453249452\n",
      "Iteration:  640 gradient_magnitude:  6.4319444656794404\n",
      "Iteration:  641 gradient_magnitude:  6.424544608096005\n",
      "Iteration:  642 gradient_magnitude:  6.417162813488447\n",
      "Iteration:  643 gradient_magnitude:  6.409799015192584\n",
      "Iteration:  644 gradient_magnitude:  6.402453146888301\n",
      "Iteration:  645 gradient_magnitude:  6.395125142597118\n",
      "Iteration:  646 gradient_magnitude:  6.387814936679786\n",
      "Iteration:  647 gradient_magnitude:  6.380522463833894\n",
      "Iteration:  648 gradient_magnitude:  6.373247659091514\n",
      "Iteration:  649 gradient_magnitude:  6.365990457816851\n",
      "Iteration:  650 gradient_magnitude:  6.3587507957039175\n",
      "Iteration:  651 gradient_magnitude:  6.351528608774239\n",
      "Iteration:  652 gradient_magnitude:  6.344323833374565\n",
      "Iteration:  653 gradient_magnitude:  6.3371364061746105\n",
      "Iteration:  654 gradient_magnitude:  6.329966264164815\n",
      "Iteration:  655 gradient_magnitude:  6.32281334465412\n",
      "Iteration:  656 gradient_magnitude:  6.315677585267766\n",
      "Iteration:  657 gradient_magnitude:  6.308558923945109\n",
      "Iteration:  658 gradient_magnitude:  6.301457298937461\n",
      "Iteration:  659 gradient_magnitude:  6.294372648805941\n",
      "Iteration:  660 gradient_magnitude:  6.2873049124193505\n",
      "Iteration:  661 gradient_magnitude:  6.2802540289520685\n",
      "Iteration:  662 gradient_magnitude:  6.273219937881964\n",
      "Iteration:  663 gradient_magnitude:  6.2662025789883185\n",
      "Iteration:  664 gradient_magnitude:  6.259201892349783\n",
      "Iteration:  665 gradient_magnitude:  6.2522178183423405\n",
      "Iteration:  666 gradient_magnitude:  6.245250297637284\n",
      "Iteration:  667 gradient_magnitude:  6.238299271199229\n",
      "Iteration:  668 gradient_magnitude:  6.23136468028412\n",
      "Iteration:  669 gradient_magnitude:  6.224446466437275\n",
      "Iteration:  670 gradient_magnitude:  6.217544571491435\n",
      "Iteration:  671 gradient_magnitude:  6.210658937564833\n",
      "Iteration:  672 gradient_magnitude:  6.203789507059283\n",
      "Iteration:  673 gradient_magnitude:  6.1969362226582785\n",
      "Iteration:  674 gradient_magnitude:  6.190099027325116\n",
      "Iteration:  675 gradient_magnitude:  6.18327786430103\n",
      "Iteration:  676 gradient_magnitude:  6.176472677103343\n",
      "Iteration:  677 gradient_magnitude:  6.16968340952363\n",
      "Iteration:  678 gradient_magnitude:  6.1629100056259105\n",
      "Iteration:  679 gradient_magnitude:  6.156152409744838\n",
      "Iteration:  680 gradient_magnitude:  6.149410566483918\n",
      "Iteration:  681 gradient_magnitude:  6.142684420713739\n",
      "Iteration:  682 gradient_magnitude:  6.135973917570216\n",
      "Iteration:  683 gradient_magnitude:  6.129279002452846\n",
      "Iteration:  684 gradient_magnitude:  6.122599621022989\n",
      "Iteration:  685 gradient_magnitude:  6.11593571920215\n",
      "Iteration:  686 gradient_magnitude:  6.109287243170287\n",
      "Iteration:  687 gradient_magnitude:  6.1026541393641285\n",
      "Iteration:  688 gradient_magnitude:  6.096036354475504\n",
      "Iteration:  689 gradient_magnitude:  6.08943383544969\n",
      "Iteration:  690 gradient_magnitude:  6.082846529483772\n",
      "Iteration:  691 gradient_magnitude:  6.076274384025018\n",
      "Iteration:  692 gradient_magnitude:  6.06971734676926\n",
      "Iteration:  693 gradient_magnitude:  6.063175365659306\n",
      "Iteration:  694 gradient_magnitude:  6.056648388883344\n",
      "Iteration:  695 gradient_magnitude:  6.050136364873377\n",
      "Iteration:  696 gradient_magnitude:  6.043639242303661\n",
      "Iteration:  697 gradient_magnitude:  6.037156970089158\n",
      "Iteration:  698 gradient_magnitude:  6.030689497384004\n",
      "Iteration:  699 gradient_magnitude:  6.024236773579986\n",
      "Iteration:  700 gradient_magnitude:  6.01779874830504\n",
      "Iteration:  701 gradient_magnitude:  6.0113753714217495\n",
      "Iteration:  702 gradient_magnitude:  6.0049665930258636\n",
      "Iteration:  703 gradient_magnitude:  5.998572363444828\n",
      "Iteration:  704 gradient_magnitude:  5.9921926332363284\n",
      "Iteration:  705 gradient_magnitude:  5.985827353186837\n",
      "Iteration:  706 gradient_magnitude:  5.979476474310184\n",
      "Iteration:  707 gradient_magnitude:  5.973139947846131\n",
      "Iteration:  708 gradient_magnitude:  5.966817725258964\n",
      "Iteration:  709 gradient_magnitude:  5.960509758236086\n",
      "Iteration:  710 gradient_magnitude:  5.954215998686639\n",
      "Iteration:  711 gradient_magnitude:  5.947936398740115\n",
      "Iteration:  712 gradient_magnitude:  5.941670910745001\n",
      "Iteration:  713 gradient_magnitude:  5.935419487267419\n",
      "Iteration:  714 gradient_magnitude:  5.929182081089781\n",
      "Iteration:  715 gradient_magnitude:  5.9229586452094605\n",
      "Iteration:  716 gradient_magnitude:  5.9167491328374675\n",
      "Iteration:  717 gradient_magnitude:  5.910553497397136\n",
      "Iteration:  718 gradient_magnitude:  5.904371692522828\n",
      "Iteration:  719 gradient_magnitude:  5.898203672058637\n",
      "Iteration:  720 gradient_magnitude:  5.892049390057114\n",
      "Iteration:  721 gradient_magnitude:  5.885908800777992\n",
      "Iteration:  722 gradient_magnitude:  5.879781858686931\n",
      "Iteration:  723 gradient_magnitude:  5.873668518454267\n",
      "Iteration:  724 gradient_magnitude:  5.867568734953771\n",
      "Iteration:  725 gradient_magnitude:  5.861482463261423\n",
      "Iteration:  726 gradient_magnitude:  5.855409658654186\n",
      "Iteration:  727 gradient_magnitude:  5.849350276608802\n",
      "Iteration:  728 gradient_magnitude:  5.843304272800591\n",
      "Iteration:  729 gradient_magnitude:  5.837271603102253\n",
      "Iteration:  730 gradient_magnitude:  5.831252223582696\n",
      "Iteration:  731 gradient_magnitude:  5.825246090505855\n",
      "Iteration:  732 gradient_magnitude:  5.819253160329536\n",
      "Iteration:  733 gradient_magnitude:  5.813273389704257\n",
      "Iteration:  734 gradient_magnitude:  5.807306735472104\n",
      "Iteration:  735 gradient_magnitude:  5.801353154665597\n",
      "Iteration:  736 gradient_magnitude:  5.795412604506562\n",
      "Iteration:  737 gradient_magnitude:  5.78948504240501\n",
      "Iteration:  738 gradient_magnitude:  5.783570425958033\n",
      "Iteration:  739 gradient_magnitude:  5.777668712948698\n",
      "Iteration:  740 gradient_magnitude:  5.771779861344955\n",
      "Iteration:  741 gradient_magnitude:  5.765903829298555\n",
      "Iteration:  742 gradient_magnitude:  5.760040575143977\n",
      "Iteration:  743 gradient_magnitude:  5.754190057397353\n",
      "Iteration:  744 gradient_magnitude:  5.748352234755416\n",
      "Iteration:  745 gradient_magnitude:  5.7425270660944445\n",
      "Iteration:  746 gradient_magnitude:  5.736714510469224\n",
      "Iteration:  747 gradient_magnitude:  5.730914527112007\n",
      "Iteration:  748 gradient_magnitude:  5.725127075431492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  749 gradient_magnitude:  5.719352115011798\n",
      "Iteration:  750 gradient_magnitude:  5.7135896056114595\n",
      "Iteration:  751 gradient_magnitude:  5.707839507162418\n",
      "Iteration:  752 gradient_magnitude:  5.702101779769027\n",
      "Iteration:  753 gradient_magnitude:  5.696376383707068\n",
      "Iteration:  754 gradient_magnitude:  5.690663279422762\n",
      "Iteration:  755 gradient_magnitude:  5.684962427531802\n",
      "Iteration:  756 gradient_magnitude:  5.679273788818382\n",
      "Iteration:  757 gradient_magnitude:  5.6735973242342475\n",
      "Iteration:  758 gradient_magnitude:  5.667932994897729\n",
      "Iteration:  759 gradient_magnitude:  5.66228076209281\n",
      "Iteration:  760 gradient_magnitude:  5.6566405872681855\n",
      "Iteration:  761 gradient_magnitude:  5.651012432036327\n",
      "Iteration:  762 gradient_magnitude:  5.645396258172568\n",
      "Iteration:  763 gradient_magnitude:  5.6397920276141775\n",
      "Iteration:  764 gradient_magnitude:  5.634199702459459\n",
      "Iteration:  765 gradient_magnitude:  5.628619244966842\n",
      "Iteration:  766 gradient_magnitude:  5.623050617553986\n",
      "Iteration:  767 gradient_magnitude:  5.617493782796892\n",
      "Iteration:  768 gradient_magnitude:  5.61194870342902\n",
      "Iteration:  769 gradient_magnitude:  5.60641534234041\n",
      "Iteration:  770 gradient_magnitude:  5.600893662576814\n",
      "Iteration:  771 gradient_magnitude:  5.595383627338829\n",
      "Iteration:  772 gradient_magnitude:  5.589885199981046\n",
      "Iteration:  773 gradient_magnitude:  5.584398344011189\n",
      "Iteration:  774 gradient_magnitude:  5.578923023089279\n",
      "Iteration:  775 gradient_magnitude:  5.57345920102679\n",
      "Iteration:  776 gradient_magnitude:  5.568006841785817\n",
      "Iteration:  777 gradient_magnitude:  5.56256590947825\n",
      "Iteration:  778 gradient_magnitude:  5.557136368364956\n",
      "Iteration:  779 gradient_magnitude:  5.551718182854957\n",
      "Iteration:  780 gradient_magnitude:  5.546311317504628\n",
      "Iteration:  781 gradient_magnitude:  5.540915737016889\n",
      "Iteration:  782 gradient_magnitude:  5.535531406240412\n",
      "Iteration:  783 gradient_magnitude:  5.530158290168826\n",
      "Iteration:  784 gradient_magnitude:  5.524796353939933\n",
      "Iteration:  785 gradient_magnitude:  5.519445562834926\n",
      "Iteration:  786 gradient_magnitude:  5.514105882277618\n",
      "Iteration:  787 gradient_magnitude:  5.508777277833666\n",
      "Iteration:  788 gradient_magnitude:  5.5034597152098135\n",
      "Iteration:  789 gradient_magnitude:  5.49815316025313\n",
      "Iteration:  790 gradient_magnitude:  5.492857578950258\n",
      "Iteration:  791 gradient_magnitude:  5.487572937426665\n",
      "Iteration:  792 gradient_magnitude:  5.482299201945902\n",
      "Iteration:  793 gradient_magnitude:  5.477036338908865\n",
      "Iteration:  794 gradient_magnitude:  5.471784314853066\n",
      "Iteration:  795 gradient_magnitude:  5.4665430964519\n",
      "Iteration:  796 gradient_magnitude:  5.461312650513934\n",
      "Iteration:  797 gradient_magnitude:  5.456092943982177\n",
      "Iteration:  798 gradient_magnitude:  5.450883943933379\n",
      "Iteration:  799 gradient_magnitude:  5.445685617577321\n",
      "Iteration:  800 gradient_magnitude:  5.440497932256111\n",
      "Iteration:  801 gradient_magnitude:  5.435320855443489\n",
      "Iteration:  802 gradient_magnitude:  5.430154354744134\n",
      "Iteration:  803 gradient_magnitude:  5.424998397892982\n",
      "Iteration:  804 gradient_magnitude:  5.419852952754531\n",
      "Iteration:  805 gradient_magnitude:  5.414717987322179\n",
      "Iteration:  806 gradient_magnitude:  5.409593469717538\n",
      "Iteration:  807 gradient_magnitude:  5.4044793681897705\n",
      "Iteration:  808 gradient_magnitude:  5.399375651114929\n",
      "Iteration:  809 gradient_magnitude:  5.3942822869952884\n",
      "Iteration:  810 gradient_magnitude:  5.389199244458697\n",
      "Iteration:  811 gradient_magnitude:  5.384126492257928\n",
      "Iteration:  812 gradient_magnitude:  5.379063999270024\n",
      "Iteration:  813 gradient_magnitude:  5.374011734495666\n",
      "Iteration:  814 gradient_magnitude:  5.368969667058529\n",
      "Iteration:  815 gradient_magnitude:  5.363937766204653\n",
      "Iteration:  816 gradient_magnitude:  5.358916001301811\n",
      "Iteration:  817 gradient_magnitude:  5.353904341838887\n",
      "Iteration:  818 gradient_magnitude:  5.348902757425254\n",
      "Iteration:  819 gradient_magnitude:  5.34391121779016\n",
      "Iteration:  820 gradient_magnitude:  5.3389296927821155\n",
      "Iteration:  821 gradient_magnitude:  5.33395815236828\n",
      "Iteration:  822 gradient_magnitude:  5.328996566633868\n",
      "Iteration:  823 gradient_magnitude:  5.32404490578154\n",
      "Iteration:  824 gradient_magnitude:  5.319103140130815\n",
      "Iteration:  825 gradient_magnitude:  5.314171240117469\n",
      "Iteration:  826 gradient_magnitude:  5.309249176292956\n",
      "Iteration:  827 gradient_magnitude:  5.304336919323818\n",
      "Iteration:  828 gradient_magnitude:  5.299434439991106\n",
      "Iteration:  829 gradient_magnitude:  5.294541709189808\n",
      "Iteration:  830 gradient_magnitude:  5.289658697928269\n",
      "Iteration:  831 gradient_magnitude:  5.284785377327627\n",
      "Iteration:  832 gradient_magnitude:  5.279921718621248\n",
      "Iteration:  833 gradient_magnitude:  5.275067693154159\n",
      "Iteration:  834 gradient_magnitude:  5.270223272382498\n",
      "Iteration:  835 gradient_magnitude:  5.2653884278729555\n",
      "Iteration:  836 gradient_magnitude:  5.260563131302224\n",
      "Iteration:  837 gradient_magnitude:  5.2557473544564495\n",
      "Iteration:  838 gradient_magnitude:  5.250941069230692\n",
      "Iteration:  839 gradient_magnitude:  5.246144247628386\n",
      "Iteration:  840 gradient_magnitude:  5.241356861760796\n",
      "Iteration:  841 gradient_magnitude:  5.236578883846493\n",
      "Iteration:  842 gradient_magnitude:  5.231810286210816\n",
      "Iteration:  843 gradient_magnitude:  5.2270510412853595\n",
      "Iteration:  844 gradient_magnitude:  5.222301121607432\n",
      "Iteration:  845 gradient_magnitude:  5.217560499819553\n",
      "Iteration:  846 gradient_magnitude:  5.212829148668925\n",
      "Iteration:  847 gradient_magnitude:  5.208107041006932\n",
      "Iteration:  848 gradient_magnitude:  5.203394149788616\n",
      "Iteration:  849 gradient_magnitude:  5.198690448072184\n",
      "Iteration:  850 gradient_magnitude:  5.193995909018495\n",
      "Iteration:  851 gradient_magnitude:  5.189310505890566\n",
      "Iteration:  852 gradient_magnitude:  5.184634212053072\n",
      "Iteration:  853 gradient_magnitude:  5.179967000971853\n",
      "Iteration:  854 gradient_magnitude:  5.175308846213425\n",
      "Iteration:  855 gradient_magnitude:  5.170659721444488\n",
      "Iteration:  856 gradient_magnitude:  5.166019600431445\n",
      "Iteration:  857 gradient_magnitude:  5.161388457039925\n",
      "Iteration:  858 gradient_magnitude:  5.15676626523429\n",
      "Iteration:  859 gradient_magnitude:  5.152152999077176\n",
      "Iteration:  860 gradient_magnitude:  5.14754863272901\n",
      "Iteration:  861 gradient_magnitude:  5.1429531404475455\n",
      "Iteration:  862 gradient_magnitude:  5.138366496587391\n",
      "Iteration:  863 gradient_magnitude:  5.133788675599552\n",
      "Iteration:  864 gradient_magnitude:  5.129219652030965\n",
      "Iteration:  865 gradient_magnitude:  5.124659400524042\n",
      "Iteration:  866 gradient_magnitude:  5.120107895816213\n",
      "Iteration:  867 gradient_magnitude:  5.115565112739477\n",
      "Iteration:  868 gradient_magnitude:  5.111031026219948\n",
      "Iteration:  869 gradient_magnitude:  5.106505611277411\n",
      "Iteration:  870 gradient_magnitude:  5.101988843024876\n",
      "Iteration:  871 gradient_magnitude:  5.097480696668141\n",
      "Iteration:  872 gradient_magnitude:  5.092981147505342\n",
      "Iteration:  873 gradient_magnitude:  5.088490170926529\n",
      "Iteration:  874 gradient_magnitude:  5.084007742413227\n",
      "Iteration:  875 gradient_magnitude:  5.079533837538002\n",
      "Iteration:  876 gradient_magnitude:  5.07506843196404\n",
      "Iteration:  877 gradient_magnitude:  5.070611501444713\n",
      "Iteration:  878 gradient_magnitude:  5.066163021823163\n",
      "Iteration:  879 gradient_magnitude:  5.0617229690318775\n",
      "Iteration:  880 gradient_magnitude:  5.057291319092269\n",
      "Iteration:  881 gradient_magnitude:  5.052868048114266\n",
      "Iteration:  882 gradient_magnitude:  5.048453132295893\n",
      "Iteration:  883 gradient_magnitude:  5.044046547922864\n",
      "Iteration:  884 gradient_magnitude:  5.039648271368172\n",
      "Iteration:  885 gradient_magnitude:  5.035258279091685\n",
      "Iteration:  886 gradient_magnitude:  5.03087654763974\n",
      "Iteration:  887 gradient_magnitude:  5.026503053644745\n",
      "Iteration:  888 gradient_magnitude:  5.022137773824776\n",
      "Iteration:  889 gradient_magnitude:  5.017780684983185\n",
      "Iteration:  890 gradient_magnitude:  5.013431764008205\n",
      "Iteration:  891 gradient_magnitude:  5.009090987872555\n",
      "Iteration:  892 gradient_magnitude:  5.004758333633055\n",
      "Iteration:  893 gradient_magnitude:  5.000433778430235\n",
      "Iteration:  894 gradient_magnitude:  4.9961172994879535\n",
      "Here are the final weights after convergence:\n",
      "[ 0.2964133  -0.1068523   0.11042425 ...  0.          0.18507002\n",
      "  0.26978528]\n"
     ]
    }
   ],
   "source": [
    "#Initialize the weights, step size and tolerance\n",
    "# Start of code\n",
    "#STUDENT: Specify the initial_weights, step_size, and tolerance\n",
    "initial_weights =  np.zeros(4501)\n",
    "step_size = 0.005\n",
    "tolerance = 5\n",
    "# end of code\n",
    "\n",
    "# Use the regression_gradient_descent function to calculate the gradient decent and store it in the variable 'final_weights'\n",
    "final_weights = gradient_descent(train_data,train_labels, initial_weights, step_size, tolerance)\n",
    "\n",
    "# end of code\n",
    "print (\"Here are the final weights after convergence:\")\n",
    "print (final_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task P4:** Write the code to extract the y-intercept $\\theta_0$ and the rest of the parameters $\\theta= \\begin{bmatrix} \\theta_1 & ... & \\theta_d \\end{bmatrix}^T$. Copy the code and print the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y intercept:  0.2964132952656125\n",
      "theta1 and theta2:  0.11042424734280551 0.9606256296966943\n"
     ]
    }
   ],
   "source": [
    "## STUDENT: CODE STARTS HERE\n",
    "## Pull out the parameters (theta_0, theta) of the logistic regression model\n",
    "theta0 = final_weights[0]\n",
    "theta = final_weights[1:]\n",
    "## STUDENT: CODE ENDS HERE\n",
    "print ('y intercept: ',theta0)\n",
    "print ('theta1 and theta2: ',theta[1],theta[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the [logistic/sigmoid function](http://en.wikipedia.org/wiki/Logistic_function) is given by\n",
    "\n",
    "$$\n",
    "f(z)=\\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "Based on the logistic model, the posterior probability\n",
    "\n",
    "$$\n",
    "P(y\\mid{\\bf x})=f(y\\; {\\bf \\tilde \\theta}^T {\\bf \\tilde x})\n",
    "$$\n",
    "\n",
    "To make a prediction, we can simply choose the label $y\\in\\{-1,+1\\}$ with the higher posterior probability.\n",
    "\n",
    "**Task P5:** Write the code to make the prediction for a given data matrix. Report the training error and test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(feature_matrix,weights):\n",
    "# Prediction made by logistic regression   \n",
    "    \n",
    "    # Input:\n",
    "    # feature_matrix: numpy array of size n by d+1, where n is the number of data points, and d+1 is the feature dimension\n",
    "    #                 note we have included the dummy feature as the first column of the feature_matrix\n",
    "    # weights: weight vector to start with, a numpy vector of dimension d+1\n",
    "    # Output:\n",
    "    # labels: predicted labels, a numpy vector of dimension n\n",
    "    \n",
    "    ## STUDENT: YOUR CODE HERE\n",
    "    predicted_labels = np.zeros(len(feature_matrix[0]))\n",
    "    \n",
    "    for i in range(len(feature_matrix)):\n",
    "        inside = np.dot(weights, feature_matrix[i])\n",
    "        predict_pos = 1 / (1 + np.exp(-1. * inside))\n",
    "        predict_neg = 1 / (1 + np.exp(inside))\n",
    "        predicted_labels[i] = 1 if predict_pos > predict_neg else 0\n",
    "        \n",
    "    return predicted_labels\n",
    "    ## STUDENT: CODE ENDS\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error:  0.0004\n",
      "Test error:  0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-92528e4315e1>:8: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  errs_train = np.sum((preds_train > 0.0) != (train_labels > 0.0))\n",
      "<ipython-input-15-92528e4315e1>:9: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  errs_test = np.sum((preds_test > 0.0) != (test_labels > 0.0))\n"
     ]
    }
   ],
   "source": [
    "# STUDENT: copy the output of this section to the solution file\n",
    "\n",
    "## Get predictions on training and test data\n",
    "preds_train = model_predict(train_data,final_weights)\n",
    "preds_test = model_predict(test_data,final_weights)\n",
    "\n",
    "## Compute errors\n",
    "errs_train = np.sum((preds_train > 0.0) != (train_labels > 0.0))\n",
    "errs_test = np.sum((preds_test > 0.0) != (test_labels > 0.0))\n",
    "\n",
    "print (\"Training error: \", float(errs_train)/len(train_labels))\n",
    "print (\"Test error: \", float(errs_test)/len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing the margin\n",
    "\n",
    "As discussed in the lecture, the logistic regression model produces not just classifications but also conditional probability estimates. \n",
    "\n",
    "We will say that `x` has **margin** `gamma` if (according to the logistic regression model) `Pr(y=1|x) > (1/2)+gamma` or `Pr(y=1|x) < (1/2)-gamma`. For example, if `Pr(y=1|x)` is 0.7 according to the logistic regression model, then the margin is 0.2. If `Pr(y=1|x)` is 0.15 according to the logistic regression model, then the margin is 0.35.\n",
    "\n",
    "**Task P6:** Implement the following function **margin_counts** that takes as input the learned weights $\\bf\\tilde\\theta$, the feature matrix (`feature_matrix`), and a value of `gamma`, and computes how many points in the data have margin at least `gamma`. Copy the code and the output plot (i.e., visualization of the test set's distribution of margin values) to the solution file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_counts(feature_matrix, weights, gamma):\n",
    "## Return number of points for which Pr(y=1) lies in [0, 0.5 - gamma) or (0.5 + gamma, 1]\n",
    "\n",
    "    # Input:\n",
    "    # feature_matrix: numpy array of size n by d+1, where n is the number of data points, and d+1 is the feature dimension\n",
    "    #                 note we have included the dummy feature as the first column of the feature_matrix\n",
    "    # weights: weight vector to start with, a numpy vector of dimension d+1\n",
    "    # gamma: the margin value\n",
    "    # Output:\n",
    "    # number of points for which Pr(y=1) lies in [0, 0.5 - gamma) or (0.5 + gamma, 1]\n",
    "    \n",
    "    ## STUDENT: YOUR CODE HERE\n",
    "    gamma_sum = 0\n",
    "    for i in range(len(feature_matrix)):\n",
    "        inside = np.dot(weights, feature_matrix[i])\n",
    "        predict_pos = 1 / (1 + np.exp(-1. * inside))\n",
    "        if (0 <= predict_pos < (0.5 - gamma)) or ((0.5 + gamma) < predict_pos <= 1):\n",
    "            gamma_sum += 1\n",
    "    \n",
    "    return gamma_sum\n",
    "\n",
    "    ## STUDENT: CODE ENDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the test set's distribution of margin values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gammas = np.arange(0,0.5,0.01)\n",
    "f = np.vectorize(lambda g: margin_counts(test_data, final_weights,g))\n",
    "plt.plot(gammas, f(gammas)/500.0, linewidth=2, color='green')\n",
    "plt.xlabel('Margin', fontsize=14)\n",
    "plt.ylabel('Fraction of points above margin', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we investigate a natural question: Are points `x` with larger margin more likely to be classified correctly?\n",
    "\n",
    "To address this, we define a function **margin_errors** that computes the fraction of points with margin at least `gamma` that are misclassified.\n",
    "\n",
    "**Task P7:** Implement the function `margin_errors` that computes the fraction of points with margin at least `gamma` that are misclassified. Copy the code and the output plot (i.e., visualization of the relationship between margin and error rate) to the solution file. What do you observe from the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_errors(feature_matrix, labels, weights, gamma):\n",
    "## Return error of predictions that lie in intervals [0, 0.5 - gamma) and (0.5 + gamma, 1]\n",
    "\n",
    "    # Input:\n",
    "    # feature_matrix: numpy array of size n by d+1, where n is the number of data points, and d+1 is the feature dimension\n",
    "    #                 note we have included the dummy feature as the first column of the feature_matrix\n",
    "    # labels: true labels y, a numpy vector of dimension n\n",
    "    # weights: weight vector to start with, a numpy vector of dimension d+1\n",
    "    # gamma: the margin value\n",
    "    # Output:\n",
    "    # error of predictions that lie in intervals [0, 0.5 - gamma) and (0.5 + gamma, 1]\n",
    "    \n",
    "    ## STUDENT: YOUR CODE HERE\n",
    "    gamma_error = 0.\n",
    "    predictions = model_predict(feature_matrix, weights)\n",
    "    for i in range(len(feature_matrix)):\n",
    "        inside = np.dot(weights, feature_matrix[i])\n",
    "        predict_pos = 1 / (1 + np.exp(-1. * inside))\n",
    "        if (0 <= predict_pos < (0.5 - gamma)) or ((0.5 + gamma) < predict_pos <= 1):\n",
    "            if predictions[i] == labels[i]:\n",
    "                gamma_error += 1.\n",
    "    \n",
    "    return gamma_error / len(predictions)\n",
    "\n",
    "    ## STUDENT: YOUR CODE ENDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the relationship between margin and error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Create grid of gamma values\n",
    "gammas = np.arange(0, 0.5, 0.01)\n",
    "\n",
    "## Compute margin_errors on test data for each value of g\n",
    "f = np.vectorize(lambda g: margin_errors(test_data, test_labels,final_weights, g))\n",
    "\n",
    "## Plot the result\n",
    "plt.plot(gammas, f(gammas), linewidth=2)\n",
    "plt.ylabel('Error rate', fontsize=14)\n",
    "plt.xlabel('Margin', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Words with large influence\n",
    "\n",
    "Finally, we attempt to partially **interpret** the logistic regression model.\n",
    "\n",
    "Which words are most important in deciding whether a sentence is positive? As a first approximation to this, we simply take the words whose coefficients in $\\theta$ have the largest positive values.\n",
    "\n",
    "Likewise, we look at the words whose coefficients in $\\theta$ have the most negative values, and we think of these as influential in negative predictions.\n",
    "\n",
    "**Task P8:** Report the top 10 positive words (i.e., words with the largest positive coefficients of $\\theta$) and the top 10 negative words (i.e., words with the most negative coefficients of $\\theta$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten influential words:\n",
      "Positive Words\n",
      "1 :  greater\n",
      "2 :  loved\n",
      "3 :  perfected\n",
      "4 :  awful\n",
      "5 :  delight\n",
      "6 :  nicely\n",
      "7 :  lovely\n",
      "8 :  function\n",
      "9 :  likes\n",
      "10 :  beautifully\n",
      "Negative Words\n",
      "1 :  greater\n",
      "2 :  disapppointment\n",
      "3 :  poorly\n",
      "4 :  wasted\n",
      "5 :  worth\n",
      "6 :  disappointment\n",
      "7 :  arepas\n",
      "8 :  sucks\n",
      "9 :  badly\n",
      "10 :  dog\n"
     ]
    }
   ],
   "source": [
    "## Convert vocabulary into a list:\n",
    "## This is a list where the i-th entry corresponds to the \n",
    "\n",
    "vocab = np.array([z[0] for z in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])])\n",
    "\n",
    "## STUDENT: YOUR CODE HERE\n",
    "sorted_words = np.argsort(final_weights)\n",
    "pos_words = []\n",
    "neg_words = []\n",
    "for i in range(10):\n",
    "    pos_words.append(vocab[sorted_words[i]])\n",
    "    neg_words.append(vocab[sorted_words[-i]])\n",
    "    \n",
    "print(\"Top ten influential words:\")\n",
    "print(\"Positive Words\")\n",
    "for i in range(10):\n",
    "    print(i + 1, \": \", pos_words[i])\n",
    "print(\"Negative Words\")\n",
    "for i in range(10):\n",
    "    print(i + 1, \": \", neg_words[i])\n",
    "\n",
    "## STUDENT: CODE ENDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. (Bonus question) Classifiers that can abstain\n",
    "\n",
    "Suppose you are building a classifier, and can tolerate an error rate of at most some value `e`. Unfortunately, every classifier you try has a higher error than this. \n",
    "\n",
    "Therefore, you decide that the classifier is allowed to occasionally **abstain**: that is, to say *\"don't know\"*. When it actually makes a prediction, it must have error rate at most `e`. And subject to this constraint, it should abstain as infrequently as possible.\n",
    "\n",
    "How would you build an abstaining classifier of this kind, starting from a logistic regression model? To get the bonus score, you need to show the following:\n",
    "\n",
    "* A general description of the method\n",
    "* Your code implementation\n",
    "* A case study to show how you can use it in practice (including necessary plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STUDENT: YOUR CODE HERE\n",
    "\n",
    "\n",
    "## STUDENT: CODE ENDS"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
